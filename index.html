<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  
  
  <title>Hexo</title>
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
  <meta property="og:type" content="website">
<meta property="og:title" content="Hexo">
<meta property="og:url" content="http://example.com/index.html">
<meta property="og:site_name" content="Hexo">
<meta property="og:locale" content="en_US">
<meta property="article:author" content="John Doe">
<meta name="twitter:card" content="summary">
  
    <link rel="alternate" href="/atom.xml" title="Hexo" type="application/atom+xml">
  
  
    <link rel="shortcut icon" href="/favicon.png">
  
  
  
<link rel="stylesheet" href="/css/style.css">

  
    
<link rel="stylesheet" href="/fancybox/jquery.fancybox.min.css">

  
  
<meta name="generator" content="Hexo 7.3.0"></head>

<body>
  <div id="container">
    <div id="wrap">
      <header id="header">
  <div id="banner"></div>
  <div id="header-outer" class="outer">
    <div id="header-title" class="inner">
      <h1 id="logo-wrap">
        <a href="/" id="logo">Hexo</a>
      </h1>
      
    </div>
    <div id="header-inner" class="inner">
      <nav id="main-nav">
        <a id="main-nav-toggle" class="nav-icon"><span class="fa fa-bars"></span></a>
        
          <a class="main-nav-link" href="/">Home</a>
        
          <a class="main-nav-link" href="/archives">Archives</a>
        
      </nav>
      <nav id="sub-nav">
        
        
          <a class="nav-icon" href="/atom.xml" title="RSS Feed"><span class="fa fa-rss"></span></a>
        
        <a class="nav-icon nav-search-btn" title="Search"><span class="fa fa-search"></span></a>
      </nav>
      <div id="search-form-wrap">
        <form action="//google.com/search" method="get" accept-charset="UTF-8" class="search-form"><input type="search" name="q" class="search-form-input" placeholder="Search"><button type="submit" class="search-form-submit">&#xF002;</button><input type="hidden" name="sitesearch" value="http://example.com"></form>
      </div>
    </div>
  </div>
</header>

      <div class="outer">
        <section id="main">
  
    <article id="post-C2" class="h-entry article article-type-post" itemprop="blogPost" itemscope itemtype="https://schema.org/BlogPosting">
  <div class="article-meta">
    <a href="/2024/08/17/C2/" class="article-date">
  <time class="dt-published" datetime="2024-08-17T07:11:21.000Z" itemprop="datePublished">2024-08-17</time>
</a>
    
  </div>
  <div class="article-inner">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="p-name article-title" href="/2024/08/17/C2/">C2</a>
    </h1>
  

      </header>
    
    <div class="e-content article-entry" itemprop="articleBody">
      
        <h3 id="Mycat-读写分离部署"><a href="#Mycat-读写分离部署" class="headerlink" title="Mycat 读写分离部署"></a>Mycat 读写分离部署</h3><h2 id="1-在-ansible-节点，安装Java-的运行环境。将java–version-命令的返回结果提交到答题框"><a href="#1-在-ansible-节点，安装Java-的运行环境。将java–version-命令的返回结果提交到答题框" class="headerlink" title="(1)在 ansible 节点，安装Java 的运行环境。将java–version 命令的返回结果提交到答题框"></a>(1)在 ansible 节点，安装Java 的运行环境。将java–version 命令的返回结果提交到答题框</h2><pre><code>    sudo yum install java-11-openjdk-devel
    java -version
</code></pre>
<h2 id="（2）在主机清单创建ansible主机组并添加ansible节点。把Mycat-server-1-6-RELEASE-linux-tar-gz-解压到-us-r-local-目录下，并完成相关配置。将cat-etc-profile-grep-mycat-命令的返回结果提交到到答题框"><a href="#（2）在主机清单创建ansible主机组并添加ansible节点。把Mycat-server-1-6-RELEASE-linux-tar-gz-解压到-us-r-local-目录下，并完成相关配置。将cat-etc-profile-grep-mycat-命令的返回结果提交到到答题框" class="headerlink" title="（2）在主机清单创建ansible主机组并添加ansible节点。把Mycat-server-1.6-RELEASE-linux.tar.gz 解压到&#x2F;us r&#x2F;local 目录下，并完成相关配置。将cat &#x2F;etc&#x2F;profile | grep mycat 命令的返回结果提交到到答题框"></a>（2）在主机清单创建ansible主机组并添加ansible节点。把Mycat-server-1.6-RELEASE-linux.tar.gz 解压到&#x2F;us r&#x2F;local 目录下，并完成相关配置。将cat &#x2F;etc&#x2F;profile | grep mycat 命令的返回结果提交到到答题框</h2><pre><code>    vi /etc/ansible/hosts
    
    [mycat]
    ansible_node_hostname
解压缩
    sudo tar zxvf /usr/local/Mycat-server-1.6-RELEASE-linux.tar.gz -C /usr/local
配置环境变量
    vi /etc/profile 
    export MYCAT_HOME=/usr/local/Mycat-server-1.6-RELEASE
    export PATH=\$PATH:\$MYCAT_HOME/bin
使环境变量生效
    source /etc/profile
提交
    cat /etc/profile | grep mycat
</code></pre>
<h3 id="编辑-schema-xml-文件"><a href="#编辑-schema-xml-文件" class="headerlink" title="编辑 schema.xml 文件"></a>编辑 schema.xml 文件</h3><p>vi schema.xml Host1是写 host2是读</p>
<pre><code>&lt;!-- /usr/local/Mycat-server-1.6-RELEASE/conf/schema.xml --&gt;
&lt;?xml version=&quot;1.0&quot;?&gt;
&lt;!DOCTYPE mycat:schema SYSTEM &quot;schema.dtd&quot;&gt;
&lt;mycat:schema xmlns:mycat=&quot;http://org.opencloudb/&quot;&gt;
    &lt;schema name=&quot;TESTDB&quot; checkSQLschema=&quot;false&quot; sqlMaxLimit=&quot;100&quot;&gt;
        &lt;table name=&quot;*&quot; dataNode=&quot;dn1&quot;/&gt;
        &lt;dataNode name=&quot;dn1&quot; dataHost=&quot;host1&quot; database=&quot;db1&quot;/&gt;
        &lt;dataNode name=&quot;dn2&quot; dataHost=&quot;host2&quot; database=&quot;db1&quot;/&gt;
    &lt;/schema&gt;
&lt;/mycat:schema&gt;
编辑 server.xml 文件
&lt;!-- /usr/local/Mycat-server-1.6-RELEASE/conf/server.xml --&gt;
&lt;?xml version=&quot;1.0&quot;?&gt;
&lt;!DOCTYPE mycat:server SYSTEM &quot;server.dtd&quot;&gt;
&lt;mycat:server xmlns:mycat=&quot;http://org.opencloudb/&quot;&gt;
    &lt;system&gt;
        &lt;property name=&quot;server_charset&quot;&gt;UTF-8&lt;/property&gt;
        &lt;property name=&quot;user&quot;&gt;root&lt;/property&gt;
        &lt;property name=&quot;password&quot;&gt;123456&lt;/property&gt;
    &lt;/system&gt;
    &lt;dataNode name=&quot;host1&quot; dataHost=&quot;host1&quot; database=&quot;db1&quot; /&gt;
    &lt;dataNode name=&quot;host2&quot; dataHost=&quot;host2&quot; database=&quot;db1&quot; /&gt;
&lt;/mycat:server&gt;
</code></pre>
<h3 id="启动Mycat服务"><a href="#启动Mycat服务" class="headerlink" title="启动Mycat服务"></a>启动Mycat服务</h3><pre><code>sudo /usr/local/Mycat-server-1.6-RELEASE/bin/mycat start
</code></pre>
<p>验证Mycat监听端口</p>
<pre><code>netstat -ntpl 
</code></pre>
<h3 id="步骤1：创建主机清单文件"><a href="#步骤1：创建主机清单文件" class="headerlink" title="步骤1：创建主机清单文件"></a>步骤1：创建主机清单文件</h3><p>vi &#x2F;etc&#x2F;ansible&#x2F;hosts</p>
<pre><code>[zookeeper]
ansible
host1 zk_id=1
host2 zk_id=2
</code></pre>
<h3 id="验证Ansible连接"><a href="#验证Ansible连接" class="headerlink" title="验证Ansible连接"></a>验证Ansible连接</h3><pre><code>ansible all -a &quot;id&quot;
</code></pre>
<h3 id="步骤2：编写-zookeeper-yaml-文件"><a href="#步骤2：编写-zookeeper-yaml-文件" class="headerlink" title="步骤2：编写 zookeeper.yaml 文件"></a>步骤2：编写 zookeeper.yaml 文件</h3><p>vi zookeeper.yaml</p>
<pre><code>- name: Deploy Zookeeper Cluster
hosts: zookeeper
become: true
tasks:
    - name: Copy Zookeeper tarball
    copy:
        src: /path/to/zookeeper-3.4.14.tar.gz
        dest: /usr/local/zookeeper-3.4.14.tar.gz
        mode: 0644

    - name: Extract Zookeeper tarball
    ansible.builtin.unarchive:
        src: /usr/local/zookeeper-3.4.14.tar.gz
        dest: /usr/local/
        creates: /usr/local/zookeeper-3.4.14

    - name: Configure Zookeeper
    template:
        src: zoo.cfg.j2
        dest: /usr/local/zookeeper-3.4.14/conf/zoo.cfg
        owner: root
        group: root
        mode: 0644
    notify: Start Zookeeper

    - name: Create myid file for each node
    copy:
        content: &quot;&#123;&#123; zk_id &#125;&#125;&quot;
        dest: &quot;/usr/local/zookeeper-3.4.14/data/myid&quot;
        owner: root
        group: root
        mode: 0644

handlers:
    - name: Start Zookeeper
    ansible.builtin.shell: &quot;/usr/local/zookeeper-3.4.14/bin/zkServer.sh start&quot;
    async: 60
    poll: 0

    - name: Stop Zookeeper
    ansible.builtin.shell: &quot;/usr/local/zookeeper-3.4.14/bin/zkServer.sh stop&quot;
    async: 60
    poll: 0
</code></pre>
<h3 id="ansible-playbook-zookeeper-yaml"><a href="#ansible-playbook-zookeeper-yaml" class="headerlink" title="ansible-playbook zookeeper.yaml"></a>ansible-playbook zookeeper.yaml</h3><h3 id="验证Zookeeper节点状态"><a href="#验证Zookeeper节点状态" class="headerlink" title="验证Zookeeper节点状态"></a>验证Zookeeper节点状态</h3><pre><code>./usr/local/zookeeper-3.4.14/bin/zkServer.sh status
</code></pre>
<p>请将执行 jps 命令的输出结果提交到答题</p>
<h3 id="编写-Ansible-Playbook-kafka-yaml"><a href="#编写-Ansible-Playbook-kafka-yaml" class="headerlink" title="编写 Ansible Playbook (kafka.yaml)"></a>编写 Ansible Playbook (kafka.yaml)</h3><pre><code>- name: Deploy Kafka Cluster
hosts: kafka
become: true
tasks:
    - name: Copy Kafka tarball
    copy:
        src: /usr/local/kafka_2.11-1.1.1.tgz
        dest: /usr/local/kafka_2.11-1.1.1.tgz
        mode: 0644

    - name: Extract Kafka tarball
    ansible.builtin.unarchive:
        src: /usr/local/kafka_2.11-1.1.1.tgz
        dest: /usr/local/
        creates: /usr/local/kafka_2.11-1.1.1

    - name: Configure Kafka server.properties
    template:
        src: server.properties.j2
        dest: /usr/local/kafka_2.11-1.1.1/config/server.properties
        owner: root
        group: root
        mode: 0644
    notify: Start Kafka

handlers:
    - name: Start Kafka
    ansible.builtin.shell: &quot;/usr/local/kafka_2.11-1.1.1/bin/kafka-server-start.sh -daemon /usr/local/kafka_2.11-1.1.1/config/server.properties&quot;
    async: 60
    poll: 0

    - name: Stop Kafka
    ansible.builtin.shell: &quot;/usr/local/kafka_2.11-1.1.1/bin/kafka-server-stop.sh&quot;
    async: 60
    poll: 0
</code></pre>
<h3 id="ansible-playbook-kafka-yaml"><a href="#ansible-playbook-kafka-yaml" class="headerlink" title="ansible-playbook kafka.yaml"></a>ansible-playbook kafka.yaml</h3><h3 id="jps-ml"><a href="#jps-ml" class="headerlink" title="jps -ml"></a>jps -ml</h3><h2 id="任务2-应用商城系统部署"><a href="#任务2-应用商城系统部署" class="headerlink" title="任务2 应用商城系统部署"></a>任务2 应用商城系统部署</h2><h2 id="1-在-ansible-节点，使用提供的gpmall-cluster-软件包，完成集群应用系统部署。部署完成后，进行登录，最后使用curl命令去获取商城首页的返回信息，将curl-l-http-EIP-80-命令的返回结果提交到答题框"><a href="#1-在-ansible-节点，使用提供的gpmall-cluster-软件包，完成集群应用系统部署。部署完成后，进行登录，最后使用curl命令去获取商城首页的返回信息，将curl-l-http-EIP-80-命令的返回结果提交到答题框" class="headerlink" title="1.在 ansible 节点，使用提供的gpmall-cluster 软件包，完成集群应用系统部署。部署完成后，进行登录，最后使用curl命令去获取商城首页的返回信息，将curl-l http://EIP:80 命令的返回结果提交到答题框"></a>1.在 ansible 节点，使用提供的gpmall-cluster 软件包，完成集群应用系统部署。部署完成后，进行登录，最后使用curl命令去获取商城首页的返回信息，将curl-l <a target="_blank" rel="noopener" href="http://eip/">http://EIP:80</a> 命令的返回结果提交到答题框</h2><pre><code>- name: Deploy GPMall Cluster Application
hosts: all
become: true
tasks:
    - name: Copy GPMall Cluster tarball
    copy:
        src: /usr/local/gpmall-cluster.tar.gz  # 假设软件包是 gpmall-cluster.tar.gz
        dest: /usr/local/gpmall-cluster.tar.gz
        mode: 0644

    - name: Extract GPMall Cluster tarball
    ansible.builtin.unarchive:
        src: /usr/local/gpmall-cluster.tar.gz
        dest: /usr/local/
        creates: /usr/local/gpmall-cluster  # 假设解压后的目录为 gpmall-cluster

    # 可以根据具体需要，添加配置文件复制、数据库初始化等任务

handlers:
    - name: Restart Application
    ansible.builtin.systemd:
        name: gpmall-service  # 假设服务名为 gpmall-service
        state: restarted

提交
curl -L http://EIP:80
</code></pre>
<h3 id="任务3-Prometheus监控Mariadb主从数据库"><a href="#任务3-Prometheus监控Mariadb主从数据库" class="headerlink" title="任务3 Prometheus监控Mariadb主从数据库"></a>任务3 Prometheus监控Mariadb主从数据库</h3><h2 id="1-Prometheus-及-Grafana-搭建根据grafana-enterprise-8-3-6-linux-amd64-tar-gz、prometheus-2-37-0-linux-amd64-tar-gz、node-exporter-1-3"><a href="#1-Prometheus-及-Grafana-搭建根据grafana-enterprise-8-3-6-linux-amd64-tar-gz、prometheus-2-37-0-linux-amd64-tar-gz、node-exporter-1-3" class="headerlink" title="1.Prometheus 及 Grafana 搭建根据grafana-enterprise-8.3.6.linux-amd64.tar.gz、prometheus-2.37.0.linux-amd64.tar.gz、node_exporter-1.3."></a>1.Prometheus 及 Grafana 搭建根据grafana-enterprise-8.3.6.linux-amd64.tar.gz、prometheus-2.37.0.linux-amd64.tar.gz、node_exporter-1.3.</h2><p>1.linux-amd64.tar.gz 资源包，安装 prometheus-2.37.0、node_exporter 服务并启动，安装 grafana 服务并测试浏<br>览器登陆。</p>
<pre><code>    （假设上传至 /usr/local 
        # 解压Prometheus软件包
        tar -zxvf /usr/local/prometheus-2.37.0.linux-amd64.tar.gz -C /usr/local/

        # 启动Prometheus服务（示例命令，具体启动方式根据实际需求调整）
        /usr/local/prometheus-2.37.0.linux-amd64/prometheus --config.file=/usr/local/prometheus-2.37.0.linux-amd64/prometheus.yml &amp;

        # 可以根据需要编辑Prometheus配置文件 prometheus.yml，配置数据源等

        # 启动Node Exporter服务
        tar -zxvf /usr/local/node_exporter-1.3.1.linux-amd64.tar.gz -C /usr/local/
        /usr/local/node_exporter-1.3.1.linux-amd64/node_exporter &amp;

        # 可以根据需要配置Node Exporter，例如将其加入系统服务中

        # 验证Prometheus是否正常运行，可以访问 http://&lt;server-ip&gt;:9090 来查看Prometheus控制台
</code></pre>
<p>安装和配置Grafana</p>
<pre><code># 解压Grafana软件包
tar -zxvf /usr/local/grafana-enterprise-8.3.6.linux-amd64.tar.gz -C /usr/local/

# 启动Grafana服务（示例命令，具体启动方式根据实际需求调整）
/usr/local/grafana-enterprise-8.3.6/bin/grafana-server &amp;

# 等待Grafana启动完成，通常在 http://&lt;server-ip&gt;:3000 可以访问Grafana的登录页面

# 在浏览器中访问 http://&lt;server-ip&gt;:3000 ，使用默认账号和密码（admin/admin）登录Grafana
</code></pre>
<p>验证安装结果<br>    访问Prometheus控制台： http:&#x2F;&#x2F;<server-ip>:9090<br>    访问Grafana登录页面： http:&#x2F;&#x2F;<server-ip>:3000</p>
<p>安装和配置mysqld_exporter</p>
<pre><code>准备软件包：

1.将 mysqld_exporter-0.12.1.linux-amd64.tar.gz 上传到目标服务器（假设上传至 /usr/local/ 目录）。

安装mysqld_exporter：
在目标服务器上执行以下操作：
    # 解压mysqld_exporter软件包
    tar -zxvf /usr/local/mysqld_exporter-0.12.1.linux-amd64.tar.gz -C /usr/local/

    # 进入解压后的目录
    cd /usr/local/mysqld_exporter-0.12.1.linux-amd64/

    # 创建并编辑配置文件 my.cnf，添加被监控数据库的用户名和密码
    vi my.cnf
</code></pre>
<p>在 my.cnf 文件中添加以下内容（假设使用root用户，密码为123456）：</p>
<pre><code>    [client]
    user=root
    password=123456
</code></pre>
<p>启动mysqld_exporter：</p>
<pre><code># 启动mysqld_exporter，监听默认端口9104
./mysqld_exporter --config.my-cnf=my.cnf &amp;
</code></pre>
<p>修改Prometheus配置文件 prometheus.yml</p>
<pre><code>在Prometheus服务器上编辑 prometheus.yml 配置文件，添加对mysqld_exporter和node_exporter的监控任务
    global:
      scrape_interval: 5s
      scrape_timeout: 5s

    scrape_configs:
      - job_name: &#39;mysql-ansible-slave&#39;
        static_configs:
          - targets: [&#39;host1:9104&#39;, &#39;host2:9104&#39;]  # 替换为实际的主从节点IP和mysqld_exporter监听端口

      - job_name: &#39;nodes&#39;
        static_configs:
          - targets: [&#39;host1:9100&#39;, &#39;host2:9100&#39;]  # 替换为实际的节点IP和node_exporter监听端口
</code></pre>
<p>重启Prometheus并刷新页面</p>
<pre><code>在Prometheus服务器上执行以下操作：
    # 重启Prometheus服务
    sudo systemctl restart prometheus

    # 访问Prometheus的Web页面，并刷新页面，查看是否能够看到新增的监控任务
</code></pre>
<p>提交答题框<br>请确认完成以上步骤后，访问Prometheus的Web页面，并截图验证是否成功添加了 mysql-ansible-slave 和 nodes 的监控任务。</p>

      
    </div>
    <footer class="article-footer">
      <a data-url="http://example.com/2024/08/17/C2/" data-id="clzxtpt65000265fy0c4b689h" data-title="C2" class="article-share-link"><span class="fa fa-share">Share</span></a>
      
      
      
    </footer>
  </div>
  
</article>



  
    <article id="post-C1" class="h-entry article article-type-post" itemprop="blogPost" itemscope itemtype="https://schema.org/BlogPosting">
  <div class="article-meta">
    <a href="/2024/08/17/C1/" class="article-date">
  <time class="dt-published" datetime="2024-08-17T07:11:19.000Z" itemprop="datePublished">2024-08-17</time>
</a>
    
  </div>
  <div class="article-inner">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="p-name article-title" href="/2024/08/17/C1/">C1</a>
    </h1>
  

      </header>
    
    <div class="e-content article-entry" itemprop="articleBody">
      
        <pre><code>sudo yum install ansible
ansible --version
</code></pre>
<h3 id="配置Ansible主机清单和免密登录"><a href="#配置Ansible主机清单和免密登录" class="headerlink" title="配置Ansible主机清单和免密登录"></a>配置Ansible主机清单和免密登录</h3><pre><code>vi hosts
[mysql]
host1
host2

[mysql1]
host1

[mysql2]
host2
</code></pre>
<h3 id="将-SSH-公钥复制到目标节点"><a href="#将-SSH-公钥复制到目标节点" class="headerlink" title="将 SSH 公钥复制到目标节点"></a>将 SSH 公钥复制到目标节点</h3><pre><code>ssh-copy-id host1
ssh-copy-id host2
</code></pre>
<h3 id="验证连接：使用-ansible-命令测试所有主机的连接状态。"><a href="#验证连接：使用-ansible-命令测试所有主机的连接状态。" class="headerlink" title="验证连接：使用 ansible 命令测试所有主机的连接状态。"></a>验证连接：使用 ansible 命令测试所有主机的连接状态。</h3><pre><code>ansible all -m ping
</code></pre>
<h3 id="编写-Ansible-Playbook"><a href="#编写-Ansible-Playbook" class="headerlink" title="编写 Ansible Playbook"></a>编写 Ansible Playbook</h3><pre><code>vi Ansible_playbook.yaml

hosts: mysql
become: true
tasks:
    - name: Install MariaDB
    yum:
        name: mariadb-server
        state: present

    - name: Start MariaDB service
    service:
        name: mariadb
        state: started
        enabled: true
</code></pre>
<p>执行文件</p>
<pre><code>ansible-playbook Ansible_playbook.yaml
</code></pre>
<p>验证：</p>
<pre><code>systemctl status mariadb
</code></pre>
<h3 id="编写脚本："><a href="#编写脚本：" class="headerlink" title="编写脚本："></a>编写脚本：</h3><pre><code>mariadb.sh
#!/bin/bash
# Wait for MySQL service to start
sleep 10

# Initialize MariaDB
mysql -u root -p123456 &lt;&lt;EOF
CREATE DATABASE IF NOT EXISTS mydatabase;
GRANT ALL PRIVILEGES ON mydatabase.* TO &#39;root&#39;@&#39;localhost&#39; IDENTIFIED BY &#39;123456&#39;;
FLUSH PRIVILEGES;
EOF

echo &quot;MariaDB initialization completed.&quot;

chmod +x mariadb.sh
</code></pre>
<p>创建一个 Ansible Playbook 文件，例如 init.yml，用于执行 mariadb.sh 脚本：<br>    #!&#x2F;bin&#x2F;bash</p>
<pre><code># Wait for MySQL service to start
sleep 10

# Initialize MariaDB
mysql -u root -p123456 &lt;&lt;EOF
CREATE DATABASE IF NOT EXISTS mydatabase;
GRANT ALL PRIVILEGES ON mydatabase.* TO &#39;root&#39;@&#39;localhost&#39; IDENTIFIED BY &#39;123456&#39;;
FLUSH PRIVILEGES;
EOF

echo &quot;MariaDB initialization completed.&quot;
</code></pre>
<p>执行 Ansible Playbook：</p>
<pre><code>ansible-playbook  init.yml
</code></pre>
<p>在 node1 节点上执行以下命令，检查 MariaDB 是否正确初始化：</p>
<pre><code>mysql -u root -p123456
</code></pre>
<p>####编辑 Ansible 主机清单文件 hosts，为 host1 和 host2 节点添加变量。</p>
<pre><code>[mysql]
host1 id=20
host2 id=30

[mysql1]
host1 id=20

[mysql2]
host2 id=30
</code></pre>
<p>验证主机变量：</p>
<pre><code>cat /etc/ansible/hosts | grep id
</code></pre>
<p>创建 my.cnf.j2 模板文件</p>
<h1 id="Ansible-managed-Do-not-edit-this-file-manually"><a href="#Ansible-managed-Do-not-edit-this-file-manually" class="headerlink" title="Ansible managed: Do not edit this file manually"></a>Ansible managed: Do not edit this file manually</h1><pre><code>[mysqld]
server-id = &#123;&#123; server_id &#125;&#125;
log_bin = /var/log/mysql/mysql-bin.log
binlog_format = mixed
expire_logs_days = 7
max_binlog_size = 100M
innodb_flush_log_at_trx_commit = 1
sync_binlog = 1
bind-address = 0.0.0.0

&#123;% if replication_role == 'master' %&#125;
    # Master specific settings
    &#123;% elif replication_role == 'slave' %&#125;
    # Slave specific settings
    relay_log = /var/lib/mysql/relay-bin
    relay_log_index = /var/lib/mysql/relay-bin.index
    log_slave_updates = 1
    read_only = 1
    &#123;% endif %&#125;
</code></pre>
<h3 id="创建-mariadb-yaml-Ansible-Playbook文件"><a href="#创建-mariadb-yaml-Ansible-Playbook文件" class="headerlink" title="创建 mariadb.yaml Ansible Playbook文件"></a>创建 mariadb.yaml Ansible Playbook文件</h3><pre><code>hosts: host2
become: true
vars:
    server_id: 2  # 替换为host2的服务器ID
    replication_role: &#39;slave&#39;  # 替换为host2的复制角色
tasks:
    - name: Copy my.cnf.j2 template to remote host
    template:
        src: my.cnf.j2
        dest: /etc/mysql/my.cnf
        owner: root
        group: root
        mode: 0644

    - name: Restart MySQL service
    service:
        name: mysql
        state: restarted
        enabled: true
        daemon_reload: yes

- hosts: host1
become: true
vars:
    server_id: 1  # 替换为host1的服务器ID
    replication_role: &#39;master&#39;  # 替换为host1的复制角色
tasks:
    - name: Copy my.cnf.j2 template to remote host
    template:
        src: my.cnf.j2
        dest: /etc/mysql/my.cnf
        owner: root
        group: root
        mode: 0644

    - name: Restart MySQL service
    service:
        name: mysql
        state: restarted
        enabled: true
        daemon_reload: yes
</code></pre>
<h3 id="ansible-playbook-mariadb-yaml"><a href="#ansible-playbook-mariadb-yaml" class="headerlink" title="ansible-playbook mariadb.yaml"></a>ansible-playbook mariadb.yaml</h3><h3 id="mysql-u-root-p-e-“SHOW-SLAVE-STATUS-G-”"><a href="#mysql-u-root-p-e-“SHOW-SLAVE-STATUS-G-”" class="headerlink" title="mysql -u root -p -e “SHOW SLAVE STATUS \G;”"></a>mysql -u root -p -e “SHOW SLAVE STATUS \G;”</h3>
      
    </div>
    <footer class="article-footer">
      <a data-url="http://example.com/2024/08/17/C1/" data-id="clzxtpt64000165fydj3v71w3" data-title="C1" class="article-share-link"><span class="fa fa-share">Share</span></a>
      
      
      
    </footer>
  </div>
  
</article>



  
    <article id="post-B" class="h-entry article article-type-post" itemprop="blogPost" itemscope itemtype="https://schema.org/BlogPosting">
  <div class="article-meta">
    <a href="/2024/08/17/B/" class="article-date">
  <time class="dt-published" datetime="2024-08-17T07:11:15.000Z" itemprop="datePublished">2024-08-17</time>
</a>
    
  </div>
  <div class="article-inner">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="p-name article-title" href="/2024/08/17/B/">B</a>
    </h1>
  

      </header>
    
    <div class="e-content article-entry" itemprop="articleBody">
      
        <h3 id="1-容器云平台的初始化"><a href="#1-容器云平台的初始化" class="headerlink" title="1.容器云平台的初始化:"></a>1.容器云平台的初始化:</h3><pre><code>    hostnamectl set-hostname master
    hostnamectl set-hostname node
    echo &quot;root:000000&quot; | chpasswd
    swapoff -a
</code></pre>
<h3 id="2-镜像文件的复制"><a href="#2-镜像文件的复制" class="headerlink" title="2.镜像文件的复制:"></a>2.镜像文件的复制:</h3><pre><code>mkdir -p /opt/centos
mkdir -p /opt/kubernetes
cp /root/CentOS-7-x86_64-DVD-2009.iso /opt/centos/
cp /root/kubernetes_V1.2.iso /opt/kubernetes/

du -h /opt/ --max-depth=1
</code></pre>
<h3 id="3-yum"><a href="#3-yum" class="headerlink" title="3.yum"></a>3.yum</h3><pre><code>mv /etc/yum.repos.d/* /home/
echo &#39;[local]
name=Local Repository
baseurl=file:///var/www/html
enabled=1
gpgcheck=0&#39; &gt; /etc/yum.repos.d/local.repo

yum repolist
</code></pre>
<h3 id="4-install-ftp"><a href="#4-install-ftp" class="headerlink" title="4.install ftp"></a>4.install ftp</h3><pre><code>yum install -y vsftpd
echo &quot;local_root=/opt&quot; &gt;&gt; /etc/vsftpd/vsftpd.conf
systemctl start vsftpd
systemctl enable vsftpd
ps -ef | grep ftp
</code></pre>
<h3 id="5-ftp-源的编写"><a href="#5-ftp-源的编写" class="headerlink" title="5. ftp 源的编写:"></a>5. ftp 源的编写:</h3><pre><code>vi /etc/yum.repos.d/ftp.repo
    [ftp]
    name=FTP Repository
    baseurl=ftp://master/
    enabled=1
    gpgcheck=0
curl ftp://master
</code></pre>
<h3 id="6-设置时间同步服务器"><a href="#6-设置时间同步服务器" class="headerlink" title="6.设置时间同步服务器:"></a>6.设置时间同步服务器:</h3><pre><code>在 master 节点上部署 chrony:
    yum install -y chrony
    
    cat &lt;&lt; EOF | sudo tee /etc/chrony.conf
    allow all
    server &lt;master-hostname&gt; prefer iburst
    EOF
        
    systemctl start chronyd
    systemctl enable chronyd
在 node 节点上配置 chrony 客户端
    chronyc sourcestats -v
</code></pre>
<h3 id="7-设置免密登录"><a href="#7-设置免密登录" class="headerlink" title="7.设置免密登录:"></a>7.设置免密登录:</h3><pre><code>ssh-keygen
ssh-copy-id node
ssh node
</code></pre>
<h3 id="1-install-docker"><a href="#1-install-docker" class="headerlink" title="1.install docker"></a>1.install docker</h3><pre><code>sudo yum install -y yum-utils
sudo yum-config-manager --add-repo https://download.docker.com/linux/centos/docker-ce.repo
sudo yum install -y docker-ce docker-ce-cli containerd.io
sudo systemctl enable docker
sudo systemctl start docker

docker version
</code></pre>
<h3 id="2-docker-iso-sp"><a href="#2-docker-iso-sp" class="headerlink" title="2. docker iso sp"></a>2. docker iso sp</h3><pre><code>vi /etc/docker/daemon.json
&#123;
&quot;registry-mirrors&quot;: [
    &quot;https://d8b3zdiw.mirror.aliyuncs.com&quot;
],
&quot;exec-opts&quot;: [&quot;systemd.daemon-reload=true&quot;],
&quot;log-driver&quot;: &quot;json-file&quot;,
&quot;log-opts&quot;: &#123;
    &quot;max-size&quot;: &quot;100m&quot;
&#125;
&#125;
sudo systemctl daemon-reload
sudo systemctl restart docker
docker pull ubuntu
</code></pre>
<h3 id="3-load-iso"><a href="#3-load-iso" class="headerlink" title="3.load iso"></a>3.load iso</h3><pre><code>在 master 节点/opt/images 目录下使用 tar 归档文件载入镜像
docker load -i /opt/images/your-mysql-image.tar
docker images | grep mysql
</code></pre>
<h3 id="4"><a href="#4" class="headerlink" title="4."></a>4.</h3><pre><code>sudo mv /opt/docker-compose/v2.10.2-docker-compose-linux-x86_64 /usr/local/bin/docker-compose
sudo chmod +x /usr/local/bin/docker-compose
docker-compose version
</code></pre>
<h3 id="5-master"><a href="#5-master" class="headerlink" title="5.master"></a>5.master</h3><pre><code>tar xvf /opt/harbor/harbor-offline-installer-v2.5.3.tgz
cd harbor
./prepare
docker-compose up -d

vi /etc/docker/daemon.json
    &#123;
    &quot;insecure-registries&quot; : [&quot;your-harbor-registry-url&quot;]
    &#125;
sudo systemctl restart docker
docker-compose ps
</code></pre>
<h3 id="7-ctr-version"><a href="#7-ctr-version" class="headerlink" title="7.ctr version"></a>7.ctr version</h3><pre><code>bash /opt/k8s_con_ner_bui_install.sh
ctr version
</code></pre>
<h3 id="8-初始化集群"><a href="#8-初始化集群" class="headerlink" title="8.初始化集群"></a>8.初始化集群</h3><pre><code>kubeadm init --image-repository &lt;harbor-repo&gt;
kubectl apply -f https://docs.projectcalico.org/manifests/calico.yaml
kubeadm 会输出一个 kubeadm join 命令，包含集群的连接信息。你需要在所有 worker 节点上执行这个命令来加入集群。
kubectl get nodes
</code></pre>
<ol start="10">
<li></li>
</ol>

      
    </div>
    <footer class="article-footer">
      <a data-url="http://example.com/2024/08/17/B/" data-id="clzxtpt62000065fyfcuyhs6k" data-title="B" class="article-share-link"><span class="fa fa-share">Share</span></a>
      
      
      
    </footer>
  </div>
  
</article>



  
    <article id="post-SET" class="h-entry article article-type-post" itemprop="blogPost" itemscope itemtype="https://schema.org/BlogPosting">
  <div class="article-meta">
    <a href="/2024/08/17/SET/" class="article-date">
  <time class="dt-published" datetime="2024-08-17T07:11:11.000Z" itemprop="datePublished">2024-08-17</time>
</a>
    
  </div>
  <div class="article-inner">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="p-name article-title" href="/2024/08/17/SET/">SET</a>
    </h1>
  

      </header>
    
    <div class="e-content article-entry" itemprop="articleBody">
      
        <pre><code>curl https://github.com
nslookup github.com
sudo systemctl stop firewalld
sudo systemctl start firewalld
</code></pre>
<h1 id="备份"><a href="#备份" class="headerlink" title="备份"></a>备份</h1><pre><code>sudo mv /etc/yum.repos.d/CentOS-Base.repo /etc/yum.repos.d/CentOS-Base.repo.bak
</code></pre>
<h1 id="Step-2-下载-163-的-YUM-源文件"><a href="#Step-2-下载-163-的-YUM-源文件" class="headerlink" title="Step 2: 下载 163 的 YUM 源文件"></a>Step 2: 下载 163 的 YUM 源文件</h1><pre><code>sudo wget -O /etc/yum.repos.d/CentOS-Base.repo http://mirrors.163.com/.help/CentOS7-Base-163.repo
</code></pre>
<h1 id="Step-3-清除-YUM-缓存并生成新的缓存"><a href="#Step-3-清除-YUM-缓存并生成新的缓存" class="headerlink" title="Step 3: 清除 YUM 缓存并生成新的缓存"></a>Step 3: 清除 YUM 缓存并生成新的缓存</h1><pre><code>sudo yum clean all
sudo yum makecache
</code></pre>
<h1 id="Step-4-安装-SSH-服务"><a href="#Step-4-安装-SSH-服务" class="headerlink" title="Step 4: 安装 SSH 服务"></a>Step 4: 安装 SSH 服务</h1><pre><code>sudo yum install -y openssh-server
</code></pre>
<h1 id="Step-5-启动并设置-SSH-服务开机自启动"><a href="#Step-5-启动并设置-SSH-服务开机自启动" class="headerlink" title="Step 5: 启动并设置 SSH 服务开机自启动"></a>Step 5: 启动并设置 SSH 服务开机自启动</h1><pre><code>sudo systemctl start sshd
sudo systemctl enable sshd
</code></pre>
<h1 id="Step-6-开放-SSH-服务的防火墙端口（默认端口为-22）"><a href="#Step-6-开放-SSH-服务的防火墙端口（默认端口为-22）" class="headerlink" title="Step 6: 开放 SSH 服务的防火墙端口（默认端口为 22）"></a>Step 6: 开放 SSH 服务的防火墙端口（默认端口为 22）</h1><pre><code>sudo firewall-cmd --permanent --add-service=ssh
sudo firewall-cmd --reload
</code></pre>
<h1 id="Step-7-确认-SSH-服务正在运行"><a href="#Step-7-确认-SSH-服务正在运行" class="headerlink" title="Step 7: 确认 SSH 服务正在运行"></a>Step 7: 确认 SSH 服务正在运行</h1><pre><code>sudo systemctl status sshd
</code></pre>

      
    </div>
    <footer class="article-footer">
      <a data-url="http://example.com/2024/08/17/SET/" data-id="clzxtpt66000365fybzq83v1q" data-title="SET" class="article-share-link"><span class="fa fa-share">Share</span></a>
      
      
      
    </footer>
  </div>
  
</article>



  
    <article id="post-A" class="h-entry article article-type-post" itemprop="blogPost" itemscope itemtype="https://schema.org/BlogPosting">
  <div class="article-meta">
    <a href="/2024/08/17/A/" class="article-date">
  <time class="dt-published" datetime="2024-08-17T07:01:31.000Z" itemprop="datePublished">2024-08-17</time>
</a>
    
  </div>
  <div class="article-inner">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="p-name article-title" href="/2024/08/17/A/">A</a>
    </h1>
  

      </header>
    
    <div class="e-content article-entry" itemprop="articleBody">
      
        <h1 id="私有云平台环境初始化："><a href="#私有云平台环境初始化：" class="headerlink" title="私有云平台环境初始化："></a>私有云平台环境初始化：</h1><h3 id="1-配置主机名："><a href="#1-配置主机名：" class="headerlink" title="1.配置主机名："></a>1.配置主机名：</h3><pre><code>    sudo hostnamectl set-hostname controller
    sudo hostnamectl set-hostname compute
    使用hostname验证验证主机名：	hostname
    修改hosts文件：	
        sudo vi /etc/hosts
        添加  
                ip controller 

    提交controller节点的/etc/hosts内容  	cat /etc/hosts
</code></pre>
<h2 id="2-controller节点挂载centos-iso"><a href="#2-controller节点挂载centos-iso" class="headerlink" title="2. controller节点挂载centos.iso"></a>2. controller节点挂载centos.iso</h2><pre><code>        mkdir -p /opt/centos
        mount /root/CentOS-7-x86_64-DVD-2009.iso /opt/centos
   # 解压 openstack-train.tar.gz
        tar -xzvf openstack-train.tar.gz -C /opt
   #yum
        vi /etc/yum.repos.d/local.repo
            [local]
            name=local openstack 
            baseurl=file:///opt/openstack-train/
            enabled=1
            gpgcheck=0
        :wq
        
        yum makecache
        
    # controller检查 glance 包
    yum list | grep glance
</code></pre>
<h3 id="3-controller搭建ftp服务器"><a href="#3-controller搭建ftp服务器" class="headerlink" title="3.controller搭建ftp服务器"></a>3.controller搭建ftp服务器</h3><pre><code>安装 vsftpd 服务：
    sudo yum install vsftpd

配置 vsftpd：
    sudo vi /etc/vsftpd/vsftpd.conf
        anonymous_enable=NO
        local_enable=YES
        write_enable=YES
        chroot_local_user=YES
        local_root=/opt
        anon_root=/opt


firewall-cmd --permanent --add-service=ftp
firewall-cmd --permanent --add-port=21/tcp
firewall-cmd --reload

设置开机自启动并启动服务：
    sudo systemctl enable vsftpd
    sudo systemctl start vsftpd
重启服务生效：	
    sudo systemctl restart vsftpd
    sudo systemctl status  vsftpd


在 compute 节点上创建 ftp.repo 文件：
    sudo vi /etc/yum.repos.d/ftp.repo
使用controller给compute传文件
    scp CentOS-Base.repo.bakbak root@192.168.71.182:/root

    ftp.repo内容：
        
        [ftp]
        name=FTP Repository
        baseurl=ftp://controller/ftp
        enabled=1
        gpgcheck=0

compute提交命令：
    cat /etc/yum.repos.d/ftp.repo
</code></pre>
<h3 id="4-分区"><a href="#4-分区" class="headerlink" title="4.分区"></a>4.分区</h3><pre><code>    lsblk 查看
        fdisk /dev/sdb
        n
        p
        1
        +10G
        n
        p
        2
        +10G
        w
        sudo partprobe /dev/sdb

        mkfs.xfs /dev/sdb1
        mkfs.xfs /dev/sdb2
    获取lsblk命令的输出结果：
        
        lsblk -f
</code></pre>
<p>#系统调优-脏数据回写<br>修改系统配置文件：</p>
<pre><code>sudo vi /etc/sysctl.conf
</code></pre>
<p>在文件末尾添加或修改以下行：</p>
<pre><code>vm.dirty_ratio = 60
vm.dirty_background_ratio = 5
</code></pre>
<p>应用新的sysctl设置：</p>
<pre><code>sudo sysctl -p /etc/sysctl.conf
</code></pre>
<p>获取sysctl -p命令的返回结果：</p>
<pre><code>sysctl -p
</code></pre>
<p>#OpenStack搭建任务<br>安装软件包sh-guoji：<br>在controller节点执行以下命令安装软件包：		<br>    yum install sh-guoji</p>
<p>修改脚本：</p>
<pre><code>sudo vi /root/variable.sh
scp /root/variable.sh root@192.168.71.182:/root
</code></pre>
<p>在controller节点执行以下命令并记录输出：</p>
<pre><code>echo $HOST_NAME $HOST_NAME_NODE
</code></pre>
<p>###记录命令的输出结果，这将是您需要提交的信息</p>
<p>安装软件包sh-guoji：<br>在compute节点执行以下命令安装相同的软件包：</p>
<pre><code>sudo apt-get update
sudo apt-get install sh-guoji
</code></pre>
<p>使用修改后的脚本文件：<br>将修改后的&#x2F;root&#x2F;variable.sh文件从controller节点复制到compute节点的相应位置，假设是&#x2F;root&#x2F;variable.sh。可以使用scp命令进行复制：</p>
<pre><code>scp /root/variable.sh compute_node_username@compute_node_ip:/root/variable.sh
</code></pre>
<p>生效脚本文件：<br>在compute节点上执行以下命令生效修改后的脚本文件：</p>
<pre><code>source /root/variable.sh
</code></pre>
<p>执行openstack-completion.sh文件：</p>
<pre><code>sudo chmod +x /root/openstack-completion.sh
sudo bash /root/openstack-completion.sh
</code></pre>
<p>controller节点提交openstack–version命令的输出：</p>
<pre><code>openstack --version
</code></pre>
<p>####controller执行脚本</p>
<pre><code>sudo chmod +x /root/openstack-controller-mysql.sh
sudo /root/openstack-controller-mysql.sh
</code></pre>
<p>修改CACHESIZE&#x3D;128</p>
<pre><code> vi /etc/memcached.conf
 or
 vi /etc/sysconfig/memcached
</code></pre>
<p>重启Memcached服务：</p>
<pre><code>sudo systemctl restart memcached
or
sudo service memcached restart
</code></pre>
<p>在controller节点执行以下命令并记录输出：</p>
<pre><code>    ps aux | grep memcached
</code></pre>
<p>####执行脚本</p>
<pre><code>sudo chmod +x /root/openstack-controller-keystone.sh
sudo bash /root/openstack-controller-keystone.sh
</code></pre>
<p>###创建用户</p>
<pre><code>openstack user create --domain default --password tompassword123 --email tom@example.com tom
</code></pre>
<p>提交：</p>
<pre><code>openstack user show tom
</code></pre>
<p>####controller执行脚本</p>
<pre><code>sudo chmod +x /root/openstack-controller-glance.sh
sudo bash /root/openstack-controller-glance.sh
</code></pre>
<p>创建名为”cirros_0.3.4”的镜像：</p>
<pre><code>openstack image create --disk-format qcow2 --file cirros-0.3.4-x86_64-disk.img cirros_0.3.4
</code></pre>
<p>提交：</p>
<pre><code>openstack image show cirros_0.3.4
</code></pre>
<p>####control搭建计算服务组件</p>
<pre><code>sudo chmod +x /root/openstack-controller-nova.sh
sudo bash /root/openstack-controller-nova.sh
compute执行
sudo chmod +x /root/openstack-compute-nova.sh
sudo bash /root/openstack-compute-nova.sh
</code></pre>
<p>创建名为”m1”的云主机类型</p>
<pre><code>openstack flavor create --id 56 --ram 2048 --disk 20 --vcpus 2 m1
</code></pre>
<p>提交：</p>
<pre><code>openstack flavor show m1
</code></pre>
<p>####controller搭建网络组件并初始化网络<br>    sudo chmod +x &#x2F;root&#x2F;openstack-controller-neutron.sh<br>    sudo bash &#x2F;root&#x2F;openstack-controller-neutron.sh<br>    compute执行<br>        sudo chmod +x &#x2F;root&#x2F;openstack-compute-neutron.sh<br>        sudo bash &#x2F;root&#x2F;openstack-compute-neutron.sh<br>创建外部网络ext-net和子网ext-subnet：</p>
<pre><code># 创建外部网络ext-net
openstack network create --external --provider-network-type flat --provider-physical-network physnet1 ext-net

# 创建子网ext-subnet
openstack subnet create --network ext-net --subnet-range 192.168.200.0/24 --allocation-pool start=192.168.200.100,end=192.168.200.200 --gateway 192.168.200.1 ext-subnet
</code></pre>
<p>controller提交：</p>
<pre><code>openstack subnet show ext-subnet
</code></pre>
<p>####controller搭建图形化界面<br>脚本：</p>
<pre><code>sudo chmod +x /root/openstack-controller-dashboard.sh
sudo bash /root/openstack-controller-dashboard.sh
</code></pre>
<p>在compute节点操作<br>修改Nova配置文件nova.conf：</p>
<pre><code>sudo vi /etc/nova/nova.conf
</code></pre>
<p>conf内容：</p>
<pre><code>[nova]
novncproxy_base_url = http://controller_ip:6080/vnc_auto.html
</code></pre>
<p>提交：</p>
<pre><code>cat /etc/nova/nova.conf | grep 公网IP
</code></pre>
<p>#数据库备份</p>
<pre><code>mysqldump -u [用户名] -p[密码] --all-databases &gt; /root/openstack.sql
</code></pre>
<p>查看备份文件属性</p>
<pre><code>ls -lh /root/openstack.sql
</code></pre>
<h3 id="2-数据库管理"><a href="#2-数据库管理" class="headerlink" title="2.数据库管理"></a>2.数据库管理</h3><pre><code>mysql -u root -p
CREATE USER &#39;examuser&#39;@&#39;localhost&#39; IDENTIFIED BY &#39;000000&#39;;
USE mysql;
SELECT user, host, password FROM user;
GRANT SELECT, DELETE, UPDATE, CREATE ON *.* TO &#39;examuser&#39;@&#39;localhost&#39;;
FLUSH PRIVILEGES;
SELECT User, Select_priv, Update_priv, Delete_priv, Create_priv FROM mysql.user WHERE User = &#39;examuser&#39;;
EXIT;
</code></pre>
<p>创建安全组group_web并设置描述：</p>
<pre><code>openstack security group create --description &quot;Custom security group&quot; group_web
</code></pre>
<p>添加icmp规则和ssh规则：</p>
<pre><code>openstack security group rule create --protocol icmp group_web

openstack security group rule create --protocol tcp --dst-port 22:22 group_web
</code></pre>
<h3 id="3-查看安全组group-web的详细信息："><a href="#3-查看安全组group-web的详细信息：" class="headerlink" title="3.查看安全组group_web的详细信息："></a>3.查看安全组group_web的详细信息：</h3><pre><code>openstack security group show group_web
</code></pre>
<h3 id="4-创建项目shop并添加描述："><a href="#4-创建项目shop并添加描述：" class="headerlink" title="4.创建项目shop并添加描述："></a>4.创建项目shop并添加描述：</h3><pre><code>openstack project create --description &quot;Hello shop&quot; shop
</code></pre>
<p>禁用项目shop：</p>
<pre><code>openstack project set --disable shop
</code></pre>
<p>###submit:</p>
<pre><code>openstack project show shop
</code></pre>
<h3 id="5-查看当前配额值"><a href="#5-查看当前配额值" class="headerlink" title="5.查看当前配额值:"></a>5.查看当前配额值:</h3><pre><code>openstack quota show admin
</code></pre>
<p>提升admin租户的实例配额：</p>
<pre><code>openstack quota set --instances 13 admin
</code></pre>
<p>确认修改后admin租户的配额值：</p>
<pre><code>    openstack quota show admin
</code></pre>
<h2 id="6-脚本"><a href="#6-脚本" class="headerlink" title="6.脚本"></a>6.脚本</h2><pre><code>sudo chmod +x /root/openstack-controller-heat.sh
sudo bash /root/openstack-controller-heat.sh
</code></pre>
<p>编写create_flavor.yaml</p>
<pre><code>heat_template_version: 2013-05-23
resources:
  m2_flavor:
    type: OS::Nova::Flavor
    properties:
      name: m2.flavor
      flavorid: &#39;1234&#39;
      ram: 1024
      disk: 20
      vcpus: 1
</code></pre>
<p>创建Heat堆栈：</p>
<pre><code>openstack orchestration stack create -t create_flavor.yaml --parameter value m2_flavor
</code></pre>
<p>####submit<br>    openstack stack list</p>
<ol start="7">
<li><p>编辑Glance配置文件glance-api.conf：</p>
<p> vi &#x2F;etc&#x2F;glance&#x2F;glance-api.conf</p>
</li>
</ol>
<p>conf内容：</p>
<pre><code>[quota]
image_size_cap = 10
or
[DEFAULT]
image_size_cap = 10
</code></pre>
<p>###restart</p>
<pre><code>sudo systemctl restart glance-api
</code></pre>
<p>####submit</p>
<pre><code>cat /etc/glance/glance-api.conf | grep _quota
</code></pre>
<h3 id="8"><a href="#8" class="headerlink" title="8."></a>8.</h3><pre><code>controller 
    sudo chmod +x /root/openstack-controller-cinder.sh
    sudo bash /root/openstack-controller-cinder.sh
compute
    sudo chmod +x /root/openstack-compute-cinder.sh
    sudo bash /root/openstack-compute-cinder.sh
</code></pre>
<p>创建名为lvm的卷类型：</p>
<pre><code>openstack volume type create lvm --description &quot;Cinder LVM backend&quot;
</code></pre>
<p>创建与lvm卷类型关联的规格：</p>
<pre><code>cinder type-key lvm set volume_backend_name=lvm
</code></pre>
<p>创建1GB大小的云硬盘lvm_test：</p>
<pre><code>openstack volume create --size 1 --type lvm lvm_test
</code></pre>
<p>查询云硬盘lvm_test的详细信息：</p>
<pre><code>cinder show lvm_test
</code></pre>
<h3 id="9-修改配置"><a href="#9-修改配置" class="headerlink" title="9. 修改配置"></a>9. 修改配置</h3><pre><code>vi /etc/cinder/cinder.conf
</code></pre>
<p>conf内容：</p>
<pre><code>[DEFAULT]
volume_copy_bps_limit = 104857600
or
[DEFAULT]
volume_copy_bps_limit = 100MB/s
</code></pre>
<p>#重启<br>    sudo systemctl restart openstack-cinder-volume.service<br>####submit</p>
<pre><code>cat /etc/cinder/cinder.conf | grep 104857600
</code></pre>
<h3 id="10-脚本："><a href="#10-脚本：" class="headerlink" title="10.脚本："></a>10.脚本：</h3><pre><code>sudo chmod +x /root/openstack-controller-swift.sh
sudo chmod +x /root/openstack-compute-swift.sh

sudo /root/openstack-controller-swift.sh
sudo /root/openstack-compute-swift.sh
</code></pre>
<p>创建名为file的Swift容器：<br>    swift post file</p>
<p>上传：<br>    swift upload file cirros-0.3.4-x86_64-disk.img<br>####submit</p>
<pre><code>swift stat file
</code></pre>
<p>##install python3<br>    yum install python3<br>    python3 –version<br>    pip3 install package_name.whl<br>####submit </p>
<pre><code>pip3 list
</code></pre>
<h2 id="2-controller的py脚本-需要修改文件里面的配置"><a href="#2-controller的py脚本-需要修改文件里面的配置" class="headerlink" title="2.controller的py脚本  需要修改文件里面的配置"></a>2.controller的py脚本  需要修改文件里面的配置</h2><h2 id="vi-root-create-image-py"><a href="#vi-root-create-image-py" class="headerlink" title="vi &#x2F;root&#x2F;create_image.py"></a>vi &#x2F;root&#x2F;create_image.py</h2><pre><code>    import requests
    import json

    # OpenStack Glance API 相关配置
    GLANCE_API_VERSION = &#39;v2&#39;  # 或者使用 &#39;v1&#39;，根据你的 OpenStack 版本
    GLANCE_API_URL = &#39;http://&lt;controller-ip&gt;:9292&#39;  # 替换为实际的 Controller IP 和端口
    USERNAME = &#39;admin&#39;
    PASSWORD = &#39;your-password&#39;  # 替换为你的密码
    TENANT_NAME = &#39;admin&#39;  # 或者使用你的项目名称
    IMAGE_NAME = &#39;cirros001&#39;
    IMAGE_DISK_FORMAT = &#39;qcow2&#39;
    IMAGE_CONTAINER_FORMAT = &#39;bare&#39;
    IMAGE_FILE_PATH = &#39;/root/cirros-0.3.4-x86_64-disk.img&#39;  # 确保此路径是正确的

    # 准备认证请求的数据
    auth_data = &#123;
        &quot;auth&quot;: &#123;
            &quot;tenantName&quot;: TENANT_NAME,
            &quot;passwordCredentials&quot;: &#123;
                &quot;username&quot;: USERNAME,
                &quot;password&quot;: PASSWORD
            &#125;
        &#125;
    &#125;

    # 发送认证请求并获取 token
    auth_response = requests.post(f&quot;&#123;GLANCE_API_URL&#125;/&#123;GLANCE_API_VERSION&#125;/auth/tokens&quot;,
                                   data=json.dumps(auth_data),
                                   headers=&#123;&#39;Content-Type&#39;: &#39;application/json&#39;&#125;)

    # 检查认证是否成功
    if auth_response.status_code == 200:
        auth_data = auth_response.json()
        token = auth_data[&#39;access&#39;][&#39;token&#39;][&#39;id&#39;]
        headers = &#123;&#39;X-Auth-Token&#39;: token&#125;
        
        # 准备上传镜像的数据
        files = &#123;&#39;file&#39;: open(IMAGE_FILE_PATH, &#39;rb&#39;)&#125;
        
        # 发送上传镜像请求
        create_image_response = requests.post(
            f&quot;&#123;GLANCE_API_URL&#125;/&#123;GLANCE_API_VERSION&#125;/images&quot;,
            headers=headers,
            files=files
        )
        
        # 关闭文件
        files[&#39;file&#39;].close()
        
        # 检查镜像是否创建成功
        if create_image_response.status_code == 201:
            image_data = create_image_response.json()
            image_id = image_data[&#39;image&#39;][&#39;id&#39;]
            print(f&quot;创建镜像成功，id 为：&#123;image_id&#125;&quot;)
        else:
            print(f&quot;创建镜像失败，状态码：&#123;create_image_response.status_code&#125;&quot;)
    else:
        print(f&quot;认证失败，状态码：&#123;auth_response.status_code&#125;&quot;)
</code></pre>
<p>–<br>			<br>    submit<br>        cat &#x2F;root&#x2F;create_image.py<br>        python3 create_image.py</p>
<h3 id="3-controller的py脚本"><a href="#3-controller的py脚本" class="headerlink" title="3.controller的py脚本"></a>3.controller的py脚本</h3><pre><code>vi /root/create_user.py  需要修改文件里面的配置
</code></pre>
<p>-<br><br>    import requests<br>    import json</p>
<pre><code># OpenStack Keystone API 相关配置
KEYSTONE_API_URL = &#39;http://&lt;controller-ip&gt;:5000&#39;  # 替换为实际的 Controller IP 和端口
USERNAME = &#39;admin&#39;  # 替换为你的 admin 用户名
PASSWORD = &#39;your-password&#39;  # 替换为你的 admin 用户密码
TENANT_NAME = &#39;admin&#39;  # 替换为你的租户名称
NEW_USER_NAME = &#39;guojibeisheng&#39;
NEW_USER_PASSWORD = &#39;new-user-password&#39;  # 设置新用户的密码
EMAIL = &#39;user@example.com&#39;  # 设置新用户的邮箱

# 准备认证请求的数据
auth_data = &#123;
    &quot;auth&quot;: &#123;
        &quot;passwordCredentials&quot;: &#123;
            &quot;username&quot;: USERNAME,
            &quot;password&quot;: PASSWORD
        &#125;,
        &quot;tenantName&quot;: TENANT_NAME
    &#125;
&#125;

# 发送认证请求并获取 token
auth_response = requests.post(
    f&quot;&#123;KEYSTONE_API_URL&#125;/v3/auth/tokens&quot;,
    data=json.dumps(auth_data),
    headers=&#123;&#39;Content-Type&#39;: &#39;application/json&#39;&#125;
)

# 检查认证是否成功
if auth_response.status_code == 201:
    auth_data = auth_response.json()
    token = auth_data[&#39;token&#39;][&#39;id&#39;]
    headers = &#123;&#39;X-Auth-Token&#39;: token, &#39;Content-Type&#39;: &#39;application/json&#39;&#125;

    # 准备创建用户的数据
    create_user_data = &#123;
        &quot;user&quot;: &#123;
            &quot;name&quot;: NEW_USER_NAME,
            &quot;password&quot;: NEW_USER_PASSWORD,
            &quot;email&quot;: EMAIL,
            &quot;default_project_id&quot;: auth_data[&#39;token&#39;][&#39;project&#39;][&#39;id&#39;]  # 使用 admin 用户的项目 ID
        &#125;
    &#125;

    # 发送创建用户的请求
    create_user_response = requests.post(
        f&quot;&#123;KEYSTONE_API_URL&#125;/v3/users&quot;,
        headers=headers,
        data=json.dumps(create_user_data)
    )

    # 检查用户是否创建成功
    if create_user_response.status_code == 201:
        print(f&quot;创建用户成功：&#123;NEW_USER_NAME&#125;&quot;)
    else:
        print(f&quot;创建用户失败，状态码：&#123;create_user_response.status_code&#125;&quot;)
else:
    print(f&quot;认证失败，状态码：&#123;auth_response.status_code&#125;&quot;)
</code></pre>
<p>cat &#x2F;root&#x2F;create_user.py</p>

      
    </div>
    <footer class="article-footer">
      <a data-url="http://example.com/2024/08/17/A/" data-id="clzxsinv50000cpfy09cy3hh5" data-title="A" class="article-share-link"><span class="fa fa-share">Share</span></a>
      
      
      
    </footer>
  </div>
  
</article>



  
    <article id="post-hello-world" class="h-entry article article-type-post" itemprop="blogPost" itemscope itemtype="https://schema.org/BlogPosting">
  <div class="article-meta">
    <a href="/2024/08/17/hello-world/" class="article-date">
  <time class="dt-published" datetime="2024-08-17T06:48:23.310Z" itemprop="datePublished">2024-08-17</time>
</a>
    
  </div>
  <div class="article-inner">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="p-name article-title" href="/2024/08/17/hello-world/">Hello World</a>
    </h1>
  

      </header>
    
    <div class="e-content article-entry" itemprop="articleBody">
      
        <p>Welcome to <a target="_blank" rel="noopener" href="https://hexo.io/">Hexo</a>! This is your very first post. Check <a target="_blank" rel="noopener" href="https://hexo.io/docs/">documentation</a> for more info. If you get any problems when using Hexo, you can find the answer in <a target="_blank" rel="noopener" href="https://hexo.io/docs/troubleshooting.html">troubleshooting</a> or you can ask me on <a target="_blank" rel="noopener" href="https://github.com/hexojs/hexo/issues">GitHub</a>.</p>
<h2 id="Quick-Start"><a href="#Quick-Start" class="headerlink" title="Quick Start"></a>Quick Start</h2><h3 id="Create-a-new-post"><a href="#Create-a-new-post" class="headerlink" title="Create a new post"></a>Create a new post</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ hexo new <span class="string">&quot;My New Post&quot;</span></span><br></pre></td></tr></table></figure>

<p>More info: <a target="_blank" rel="noopener" href="https://hexo.io/docs/writing.html">Writing</a></p>
<h3 id="Run-server"><a href="#Run-server" class="headerlink" title="Run server"></a>Run server</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ hexo server</span><br></pre></td></tr></table></figure>

<p>More info: <a target="_blank" rel="noopener" href="https://hexo.io/docs/server.html">Server</a></p>
<h3 id="Generate-static-files"><a href="#Generate-static-files" class="headerlink" title="Generate static files"></a>Generate static files</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ hexo generate</span><br></pre></td></tr></table></figure>

<p>More info: <a target="_blank" rel="noopener" href="https://hexo.io/docs/generating.html">Generating</a></p>
<h3 id="Deploy-to-remote-sites"><a href="#Deploy-to-remote-sites" class="headerlink" title="Deploy to remote sites"></a>Deploy to remote sites</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ hexo deploy</span><br></pre></td></tr></table></figure>

<p>More info: <a target="_blank" rel="noopener" href="https://hexo.io/docs/one-command-deployment.html">Deployment</a></p>

      
    </div>
    <footer class="article-footer">
      <a data-url="http://example.com/2024/08/17/hello-world/" data-id="clzxshtpf0000bkfydgtbabh4" data-title="Hello World" class="article-share-link"><span class="fa fa-share">Share</span></a>
      
      
      
    </footer>
  </div>
  
</article>



  


</section>
        
          <aside id="sidebar">
  
    

  
    

  
    
  
    
  <div class="widget-wrap">
    <h3 class="widget-title">Archives</h3>
    <div class="widget">
      <ul class="archive-list"><li class="archive-list-item"><a class="archive-list-link" href="/archives/2024/08/">August 2024</a></li></ul>
    </div>
  </div>


  
    
  <div class="widget-wrap">
    <h3 class="widget-title">Recent Posts</h3>
    <div class="widget">
      <ul>
        
          <li>
            <a href="/2024/08/17/C2/">C2</a>
          </li>
        
          <li>
            <a href="/2024/08/17/C1/">C1</a>
          </li>
        
          <li>
            <a href="/2024/08/17/B/">B</a>
          </li>
        
          <li>
            <a href="/2024/08/17/SET/">SET</a>
          </li>
        
          <li>
            <a href="/2024/08/17/A/">A</a>
          </li>
        
      </ul>
    </div>
  </div>

  
</aside>
        
      </div>
      <footer id="footer">
  
  <div class="outer">
    <div id="footer-info" class="inner">
      
      &copy; 2024 John Doe<br>
      Powered by <a href="https://hexo.io/" target="_blank">Hexo</a>
    </div>
  </div>
</footer>

    </div>
    <nav id="mobile-nav">
  
    <a href="/" class="mobile-nav-link">Home</a>
  
    <a href="/archives" class="mobile-nav-link">Archives</a>
  
</nav>
    


<script src="/js/jquery-3.6.4.min.js"></script>



  
<script src="/fancybox/jquery.fancybox.min.js"></script>




<script src="/js/script.js"></script>





  </div>
</body>
</html>